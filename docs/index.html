<!DOCTYPE html>
<meta charset="utf-8">

<html>

<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
    <title>Gaussian Embbeds and Masks</title>
    <meta property="og:description" content="Neural Implicit Surface Evolution"/>
    <meta property="og:image" itemprop="image" content="https://dsilvavinicius.github.io/nise/assets/representative.jpg">
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
    <link rel='stylesheet' type='text/css' href='styles.css'/>

    <!--meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:creator" content="@dsilvavinicius">
    <meta name="twitter:title" content="Neural Implicit Surface Evolution">
    <meta name="twitter:description" content="A new paper which uses differential equations to introduce a time dimension for Neural Implicits, allowing animation.">
    <meta name="twitter:image" content="https://dsilvavinicius.github.io/nise/assets/representative.jpg"-->
</head>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<body>
<div class="container">
    <div class="paper-title">
        <h1>Gaussian Embbeds and Masks</h1>
    </div>

    <div id="authors">
        <div class="author-row">
            <div class="col-3 text-center"><a href="https://mteixeiratecgraf.github.io">Marco Teixeira</a><sup>1</sup></div>
            <div class="col-3 text-center"><a href="https://dsilvavinicius.github.io">Vinícius da Silva</a><sup>1</sup></div>
            <div class="col-3 text-center"><a href="http://www.inf.puc-rio.br/~lopes">Alberto Raposo</a><sup>1</sup></div>
        </div>
        <!--div class="author-row">
            <div class="col-3 text-center"><a href="https://schardong.github.io">Guilherme Schardong</a><sup>3</sup></div>
            <div class="col-3 text-center"><a href="https://www.lschirmer.com/">Luiz Schirmer</a><sup>4</sup></div>
            <div class="col-3 text-center"><a href="https://lvelho.impa.br">Luiz Velho</a><sup>1</sup></div>
        </div-->

        <div class="affil-row">
            <div class="col-4 text-center"><sup>1</sup>PUC-Rio</div>
            <div class="col-4 text-center"><sup>2</sup>IMPA</div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
                <a class="paper-btn" href="https://visgraf.impa.br/papers/i4d-iccv2023-reb.pdf">
                    <span class="material-icons"> description </span>
                    Paper and supplementary
                </a>
                <a class="paper-btn" href="https://youtu.be/8NqwLkhaRBU">
                    <span class="material-icons"> videocam </span>
                    Video
                </a>
                <!--a class="paper-btn" href="https://github.com/dsilvavinicius/nise">
                    <span class="material-icons"> code </span>
                    Code
                </a-->
            </div>
        </div>
    </div>

        <!-- <div style="clear: both"> -->
        <!--     <div class="paper-btn-parent"> -->
        <!--         <a class="paper-btn" href="https://visgraf.impa.br/papers/i4d-iccv2023-reb.pdf"> -->
        <!--             <span class="material-icons"> description </span> -->
        <!--             Paper, supplementary -->
        <!--         </a> -->
        <!--         <a class="paper-btn" href="https://youtu.be/8NqwLkhaRBU"> -->
        <!--             <span class="material-icons"> videocam </span> -->
        <!--             Video -->
        <!--         </a> -->
        <!--         <a class="paper-btn" href="https://dsilvavinicius.github.io/nise"> -->
        <!--             <span class="material-icons"> code </span> -->
        <!--             Code (Soon) -->
        <!--         </a> -->
        <!--     </div> -->
        <!-- </div> -->
    </div>

    <section id="teaser-videos">
        <!--
        <figure style="width: 33%; float: left">
            <p class="caption_bold">
                Coarse
            </p>
        </figure>

        <figure style="width: 33%; float: left">
            <p class="caption_bold">
                Neural Implicit Normal Mapping
            </p>
        </figure>

        <figure style="width: 33%; float: left">
            <p class="caption_bold">
                Baseline
            </p>
        </figure>

docs\assets\ffmpeg-7.1.1-essentials_build\ffmpeg-7.1.1-essentials_build\bin\ffmpeg -i docs\assets\ours.mp4 -vf scale=iw/2:ih/2 -b:v 1M -c:a copy docs\assets\ours_low.mp4
docs\assets\ffmpeg-7.1.1-essentials_build\ffmpeg-7.1.1-essentials_build\bin\ffmpeg -i docs\assets\ours_50000_0.gif -vf "scale=iw:ih" -r 40 docs\assets\ours_50000_0_low.gif
docs\assets\ffmpeg-7.1.1-essentials_build\ffmpeg-7.1.1-essentials_build\bin\ffmpeg -i docs\assets\ours_50000_7.gif -vf "scale=iw/2:ih/2" -r 10 docs\assets\ours_50000_7_low.gif
docs\assets\ffmpeg-7.1.1-essentials_build\ffmpeg-7.1.1-essentials_build\bin\ffmpeg -i docs\assets\truckCombination.gif -vf "scale=iw/2:ih/2" -r 10 docs\assets\truckCombination_low.gif
docs\assets\ffmpeg-7.1.1-essentials_build\ffmpeg-7.1.1-essentials_build\bin\ffmpeg -i docs\assets\compare_legob_0.gif -vf "scale=iw/2:ih/2" -r 10 docs\assets\compare_legob_0_low.gif
docs\assets\ffmpeg-7.1.1-essentials_build\ffmpeg-7.1.1-essentials_build\bin\ffmpeg -i docs\assets\compare_chair_0.gif -vf "scale=iw/2:ih/2" -r 10 docs\assets\compare_chair_0_low.gif
docs\assets\ffmpeg-7.1.1-essentials_build\ffmpeg-7.1.1-essentials_build\bin\ffmpeg -i docs\assets\compare_mic_0.gif -vf "scale=iw/2:ih/2" -r 10 docs\assets\compare_mic_0low.gif

docs\assets\ffmpeg-7.1.1-essentials_build\ffmpeg-7.1.1-essentials_build\bin\ffmpeg -i docs\assets\compare_mic_0.gif -movflags faststart -pix_fmt yuv420p docs\assets\compare_mic_0.mp4
        -->

        <figure style="width: 100%; float: left">
            <video class="centered" width="100%" autoplay muted loop playsinline>
                <source src="assets/truckCombination.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <!--
        <img style="width: 100%; float: left" src="assets/truckCombination.gif"/>
        figure style="width: 100%; float: left">
            <video class="centered" width="100%" autoplay muted loop playsinline>
                <source src="assets/teaser_720.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure-->

        <figure style="width: 100%; float: left">
            <p class="caption_justify">
                Comparison between 3DGS and our occlusion-aware method on the Truck scene.
                Foreground occlusions in 3DGS cause distortions and loss of detail.
                Our approach filters them out, producing a cleaner and sharper reconstruction.


            </p>
        </figure>
        <figure style="width: 50%; float: left">
            <video class="centered" width="100%" autoplay muted loop playsinline>
                <source src="assets/compare_mic_0.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="width: 50%; float: left">
            <video class="centered" width="100%" autoplay muted loop playsinline>
                <source src="assets/compare_chair_0.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <!--img style="width: 50%; float: left" src="assets/compare_mic_0.gif"/><img style="width: 50%; float: left" src="assets/compare_chair_0.gif"/-->
        <figure style="width: 100%; float: left">
            <p class="caption_justify">
                Side-by-side comparison between 3DGS and our method on Chair and Mic scenes.
                Dynamic foreground elements degrade 3DGS reconstructions with ghosting and blur.
                Our model handles occlusions better, preserving structure and visual clarity.
            </p>
        </figure>
    </section>

    <section id="news">
        <h2>News</h2>
        <hr>
        <div class="row">
            <div><span class="material-icons"> description </span> [Jun 15th 2025] Page online.</div>
        </div>
    </section>

    <section id="abstract">
        <h2>Abstract</h2>
        <hr>
        <p>
            This work explores the use of Gaussian Splatting techniques for neural rendering in uncontrolled environments, incorporating appearance embeddings and occlusion masks to enhance robustness.
            We propose an extended pipeline that integrates semantic segmentation—leveraging foundation models such as Segment Anything—to identify and suppress occluded regions, thereby improving visual consistency across views.

            Appearance embeddings are learned to guide view-dependent effects and sharpen the correspondence between images under complex lighting and occlusions.
            Our approach demonstrates improved stability and visual quality in multi-view synthesis tasks, particularly in real-world scenes with background clutter and partial visibility.

            Experiments show that combining masks with appearance cues leads to more coherent reconstructions and faster convergence, especially in comparison with baseline splatting methods.


        </p>
    </section>

    <section id="overview">
        <h2>Overview</h2>
        <hr>

        <p>
            The proposed method improves the robustness of <b>Gaussian Splatting</b> in real-world (in-the-wild) scenes by
            combining semantic segmentation masks and appearance embeddings. The pipeline consists of three main stages: <b>Preparation</b>,
            <b>Appearance Modeling</b>, and <b>Occlusion-Aware Rendering</b>.
        </p>


        <p style="text-align: center;">
            <img src="assets/arquitetura_v2.png" width="100%" alt="Pipeline diagram"
                 style="border: 1px solid #333; border-radius: 8px; margin-top: 1em;">
        </p>

        <h3>Preparation</h3>
        <p>
            Input images are processed using <b>SAM2</b> (Segment Anything v2), generating binary <i>semantic masks</i> that
            highlight foreground objects and help isolate occluded regions. These masks are used both during training and
            evaluation to condition the loss and filtering stages.
        </p>

        <h3>Appearance</h3>
        <p>
            Each 3D Gaussian \( \mathcal{P}_i \) is defined by a position and orientation in space. Appearance modeling is
            conditioned by:
        </p>

        <ul>
            <li>
                <b>Splat position encoding</b>: encoded as \( \gamma_x \) and used as input to a small MLP
            </li>
            <li>
                <b>Learned appearance vector</b>: \( \varepsilon^{(a)}_i \), specific to each Gaussian
            </li>
            <li>
                Both inputs are fused in a <b>view-dependent MLP</b> that predicts color for each splat
            </li>
        </ul>

        <h3>Occlusion-aware Rendering</h3>
        <p>
            The final rendered image is computed via standard Gaussian Splatting, but the loss is conditioned on the <b>semantic
            masks</b> to ignore occluded or irrelevant regions. The rendering loss is defined as:
        </p>

        <p style="text-align: center;">
            \( \mathcal{L} = \frac{1}{N} \min_{\theta \in \Theta} \sum_i \mathcal{L}_c(x_i, \mathcal{G}(S, C_{\theta_i})) \)
        </p>

        <p>
            where \( x_i \) are input views, \( S \) the segmentation masks, and \( C_{\theta_i} \) the color predicted via
            the appearance MLP. This formulation leads to better optimization by reducing the influence of background
            clutter and occlusion.
        </p>

        <h3>Gaussian Attributes</h3>
        <p>Each 3D Gaussian \( \mathcal{P}_i \) is parameterized by:</p>
        <ul>
            <li><b>Position</b>: \( \mu_i \in \mathbb{R}^3 \)</li>
            <li><b>Covariance matrix</b>: \( \Sigma_i \in \mathbb{R}^{3 \times 3} \)</li>
            <li><b>Opacity and scale</b>: influencing splatting and visibility</li>
            <li><b>Appearance vector</b>: \( \varepsilon_i^{(a)} \in \mathbb{R}^d \), used for view-dependent color
                prediction
            </li>
        </ul>

        <!--h3>Results</h3>
        <p>
            The pipeline yields improved rendering quality in challenging scenarios, especially in scenes with partial
            occlusion or moving subjects. Experiments report consistent gains in <b>PSNR</b>, <b>SSIM</b>, and <b>LPIPS</b>
            over the baseline.
        </p-->

    </section>

<section id="results">
        <h2>Results</h2>
        <hr>

        <div>
            <div class="col-2 text-center">
                <h3>Truck Scene</h3>
                <hr>

                <figure>
                    <video class="centered" width="100%" autoplay muted loop playsinline>
                        <source src="assets/ours2_low.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </figure>
            </div>

            <div class="col-2 text-center">
                <h3>Truck: No Obstruction</h3>
                <hr>


                <figure>
                    <video class="centered" width="100%" autoplay muted loop playsinline>
                        <source src="assets/ours_low.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </figure>
            </div>
        </div>

        <div>
            <div class="col-2 text-center">
                <h3>Mic: Original Appearance</h3>
                <hr>

                <figure>
                    <video class="centered" width="100%" autoplay muted loop playsinline>
                        <source src="assets/ours_50000_0.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </figure>
            </div>

            <div class="col-2 text-center">
                <h3>Mic: Original Appearance</h3>
                <hr>

                <figure>
                    <video class="centered" width="100%" autoplay muted loop playsinline>
                        <source src="assets/ours_50000_7.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </figure>
            </div>
        </div>
    </section>

    <!--section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="https://visgraf.impa.br/papers/i4d-iccv2023-reb.pdf"><img class="screenshot" src="assets/paper-thumbnail.png"></a>
            </div>
            <div style="width: 60%">
                <p><b>Neural Implicit Surface Evolution</b></p>
                <p>Tiago Novello, Vinícius da Silva, Guilherme Schardong, Luiz Schirmer, Hélio Lopes and Luiz Velho</p>

                <div><span class="material-icons"> description </span><a href="https://visgraf.impa.br/papers/i4d-iccv2023-reb.pdf"> Paper preprint (PDF)</a></div>
                <!--<div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2201.09636"> arXiv version</a></div>-- >
                <div><span class="material-icons"> insert_comment </span><a href="assets/novello2023neural.bib"> BibTeX</a></div>
                <div><span class="material-icons"> videocam </span><a href="https://youtu.be/8NqwLkhaRBU"> Video</a></div>

                <p>Please send feedback and questions to <a href="https://sites.google.com/site/tiagonovellodebrito">Tiago Novello</a>.</p>
            </div>
        </div>
    </section-->

    <!--section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre style="background-color:#121212;"><code>@InProceedings{Novello_2023_ICCV,
    author = {Novello, Tiago and da Silva, Vin\'icius and Schardong, Guilherme and Schirmer, Luiz and
            Lopes, H\'elio and Velho, Luiz},
    title = {Neural Implicit Surface Evolution},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month = {October},
    year = {2023},
    url = {https://openaccess.thecvf.com/content/ICCV2023/html/Novello_Neural_Implicit_Surface_Evolution_ICCV_2023_paper.html},
    pages = {14279-14289}
}
</code></pre>
    </section-->

    <section id="acknowledgements">
        <h2>Acknowledgements</h2>
        <hr>
        <div class="row">
            <p>
            We would like to thank
            <a href="https://tovacinni.github.io">Towaki Takikawa</a>,
            <a href="https://joeylitalien.github.io">Joey Litalien</a>,
            <a href="https://kangxue.org/">Kangxue Yin</a>,
            <a href="https://scholar.google.de/citations?user=rFd-DiAAAAAJ">Karsten Kreis</a>,
            <a href="https://research.nvidia.com/person/charles-loop">Charles Loop</a>,
            <a href="http://www.cim.mcgill.ca/~derek/">Derek Nowrouzezahrai</a>,
            <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>,
            <a href="https://casual-effects.com/">Morgan McGuire</a> and
            <a href="https://www.cs.toronto.edu/~fidler/">Sanja Fidler</a>
            for licensing the code of the paper <a href="https://nv-tlabs.github.io/nglod/">Neural Geometric Level of Detail:
                Real-time Rendering with Implicit 3D Surfaces</a> and project page under the <a href=https://opensource.org/licenses/MIT>MIT License</a>. This website is based on that page.
            <br/>
            <br/>
            <em>We also thank the <a href="https://graphics.stanford.edu">Stanford Computer Graphics Laboratory</a> for the Bunny, Dragon, Armadillo, and Happy Buddha, acquired through the <a href="http://graphics.stanford.edu/data/3Dscanrep/">Stanford 3D scan repository</a>. Finally, we thank <a href="https://www.cs.cmu.edu/~kmcrane/Projects/ModelRepository/">Keenan Crane</a> for the Spot and Bob models.
            </p>
        </div>
    </section>
</div>
</body>

</html>